{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Q2"
      ],
      "metadata": {
        "id": "WQDWDqq2iJzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize,LabelBinarizer\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import PIL\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader,random_split,Dataset\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,multilabel_confusion_matrix"
      ],
      "metadata": {
        "id": "FcTAZe_JjW9O"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install wandb -q\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "o7OR6V9PxsER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize WandB \n",
        "wandb.init(name='test_run', \n",
        "           project='ML_Assignment2',\n",
        "           notes='This is a test run', \n",
        "           tags=['Fashion MNIST', 'Test Run'],\n",
        "           entity='helloadi19138')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "nngeYCSXyhA9",
        "outputId": "77482624-5107-4443-b66d-ec23bb9e5909"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220504_204013-i9s4x2kj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/helloadi19138/ML_Assignment2/runs/i9s4x2kj\" target=\"_blank\">test_run</a></strong> to <a href=\"https://wandb.ai/helloadi19138/ML_Assignment2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/helloadi19138/ML_Assignment2/runs/i9s4x2kj?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fbf4bc15210>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "use_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "vgg16.to(device)\n",
        "vgg16\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPv2DgvNPBKO",
        "outputId": "ca3654a2-8b37-42c3-dc14-50f5d08071f5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# Freeze training for all layers\n",
        "for param in vgg16.features.parameters():\n",
        "    param.require_grad = False\n",
        "\n",
        "# Newly created modules have require_grad=True by default\n",
        "num_features = vgg16.classifier[6].in_features\n",
        "features = list(vgg16.classifier.children())[:-1] # Remove last layer\n",
        "features.extend([nn.Linear(num_features, 10)]) # Add custom layer with 10 outputs\n",
        "vgg16.classifier = nn.Sequential(*features) # Replace the model classifier\n",
        "vgg16\n"
      ],
      "metadata": {
        "id": "GiLZxDxbPI5u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a09dff66-e360-4056-e5e0-9f15ac0141de"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hvF6gf5r2PON",
        "outputId": "f442963a-07c3-46d3-e2cb-652f2e573136",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "# Define Optimizer and Loss Function\n",
        "loss = nn.CrossEntropyLoss()\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# wandb.config={\n",
        "#     \"learning_rate\": 0.001,\n",
        "#     \"momentum\": 0.9,\n",
        "#     \"batch_size\": 32,\n",
        "#     \"epochs\":20\n",
        "# }"
      ],
      "metadata": {
        "id": "8zZPcG9EMNdc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_valid = pd.read_csv('/content/drive/MyDrive/fashion-mnist_train.csv').iloc[:1000,:]\n",
        "lb=LabelBinarizer()\n",
        "lb.fit(train_valid['label'])\n",
        "\n",
        "test = pd.read_csv('/content/drive/MyDrive/fashion-mnist_test.csv').iloc[:100,:]\n",
        "train, valid = train_test_split(train_valid, test_size=0.2,shuffle=True)\n",
        "\n",
        "train_x, train_y = np.array(train.iloc[:,1:]).astype(np.uint8).reshape(train.shape[0],28,28), lb.transform(train['label'].values).astype(np.float64)\n",
        "valid_x, valid_y = np.array(valid.iloc[:,1:]).astype(np.uint8).reshape(valid.shape[0],28,28), lb.transform(valid['label'].values).astype(np.float64)\n",
        "test_x, test_y = np.array(test.iloc[:,1:]).astype(np.uint8).reshape(test.shape[0],28,28), lb.transform(test['label'].values).astype(np.float64)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('Train set', train_x.shape, train_y.shape)\n",
        "print('Valid set', valid_x.shape, valid_y.shape)\n",
        "print('Test set',test_x.shape, test_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vnePdcJiNOo",
        "outputId": "6e8b5847-fb8c-4d50-dd39-6918620e8832"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set (800, 28, 28) (800, 10)\n",
            "Valid set (200, 28, 28) (200, 10)\n",
            "Test set (100, 28, 28) (100, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_transform=transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.RandomResizedCrop(size=256),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "test_transform=transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.RandomResizedCrop(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "class FashionMNIST(Data.Dataset):  \n",
        "\n",
        "  def __init__(self, X, y,transform=None):\n",
        "    'Initialization'\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.transform=transform\n",
        "\n",
        "  def __len__(self):\n",
        "    'Denotes the total number of samples'\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    'Generates one sample of data'\n",
        "    # Select sample\n",
        "    image,label = self.X[index],self.y[index]\n",
        "    X = self.transform(image)\n",
        "    X=X.repeat(3,1,1)\n",
        "    X=self.norm(X)    \n",
        "    return X,label\n",
        "\n",
        "  norm=transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
        "\n",
        "train_loader = Data.DataLoader(FashionMNIST(train_x,train_y,train_transform), batch_size = 32, shuffle = True)\n",
        "valid_loader = Data.DataLoader(FashionMNIST(valid_x,valid_y,test_transform), batch_size = 32, shuffle = True)\n",
        "test_loader = Data.DataLoader(FashionMNIST(test_x,test_y,test_transform), shuffle = True)"
      ],
      "metadata": {
        "id": "mhGNgY8kjQzI"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features, train_labels = next(iter(train_loader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKFAD0p6j4nJ",
        "outputId": "cf429f6e-e566-4c80-c410-8433a7a54552"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature batch shape: torch.Size([32, 3, 224, 224])\n",
            "Labels batch shape: torch.Size([32, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "  model.to(device)\n",
        "  start = time.time()\n",
        "  history = []\n",
        "  best_acc = 0.0\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    epoch_start = time.time()\n",
        "    print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "    # Set to training mode\n",
        "    model.train(True)\n",
        "\n",
        "    # Loss and Accuracy within the epoch\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0        \n",
        "    val_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "      # print(\"i=\",i)\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      \n",
        "      # Clean existing gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Forward pass - compute outputs on input data using the model\n",
        "      outputs = model(inputs)\n",
        "      # print(\"forward prop out shape\",outputs.shape)\n",
        "      \n",
        "      \n",
        "      # Compute loss\n",
        "      loss = loss_criterion(outputs, labels)\n",
        "      # print(\"Computed loss= \",loss)\n",
        "      \n",
        "      # Backpropagate the gradients\n",
        "      loss.backward()\n",
        "      # print(\"Backprop\")\n",
        "      \n",
        "      # Update the parameters\n",
        "      optimizer.step()\n",
        "      # print(\"Updation\")\n",
        "\n",
        "      train_loss += loss.item() * inputs.size(0)\n",
        "      # Compute the accuracy\n",
        "      ret, predictions = torch.max(outputs.data, 1)  #predictions= indices of max prob value in every row\n",
        "      # print(predictions)\n",
        "      _,label_max_idx = torch.max(labels.data,1)\n",
        "      # print(label_max_idx)\n",
        "      correct_counts = predictions.eq(label_max_idx)\n",
        "      \n",
        "      # Convert correct_counts to float and then compute the mean\n",
        "      acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "      \n",
        "      # Compute total accuracy in the whole batch and add to train_acc\n",
        "      train_acc += acc.item() * inputs.size(0)\n",
        "      \n",
        "      # print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
        "\n",
        "    # Validation - No gradient tracking needed\n",
        "    with torch.no_grad():\n",
        "      # Set to evaluation mode\n",
        "      model.eval()\n",
        "\n",
        "      # Validation loop\n",
        "      for j, (inputs, labels) in enumerate(valid_loader):\n",
        "          # print(\"j=\",j)\n",
        "          # if use_gpu:\n",
        "          #   inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
        "          # else:\n",
        "          #   inputs, labels = Variable(inputs), Variable(labels)\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          # Forward pass - compute outputs on input data using the model\n",
        "          outputs = model(inputs)\n",
        "\n",
        "          # Compute loss\n",
        "          loss = loss_criterion(outputs, labels)\n",
        "\n",
        "          # Compute the total loss for the batch and add it to valid_loss\n",
        "          val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "          # Calculate validation accuracy\n",
        "          ret, predictions = torch.max(outputs.data, 1)\n",
        "          _,label_max_idx = torch.max(labels.data,1)\n",
        "          # print(label_max_idx)\n",
        "          correct_counts = predictions.eq(label_max_idx)\n",
        "\n",
        "          # Convert correct_counts to float and then compute the mean\n",
        "          acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "          # Compute total accuracy in the whole batch and add to valid_acc\n",
        "          val_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "          # print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "      \n",
        "    # Find average training loss and training accuracy\n",
        "    avg_train_loss = train_loss/train_x.shape[0] \n",
        "    avg_train_acc = train_acc/train_x.shape[0] \n",
        "\n",
        "    # Find average training loss and training accuracy\n",
        "    avg_valid_loss = val_loss/valid_x.shape[0]\n",
        "    avg_valid_acc = val_acc/valid_x.shape[0]\n",
        "\n",
        "    history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
        "            \n",
        "    epoch_end = time.time()\n",
        "    # Log the loss and accuracy values at the end of each epoch\n",
        "    # wandb.log({\n",
        "    #     \"Epoch\": epoch,\n",
        "    #     \"Train Loss\": avg_train_loss,\n",
        "    #     \"Train Acc\": avg_train_acc,\n",
        "    #     \"Valid Loss\": avg_valid_loss,\n",
        "    #     \"Valid Acc\": avg_valid_acc})\n",
        "\n",
        "    print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch+1, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))\n",
        "    \n",
        "    # Save if the model has best accuracy till now\n",
        "    #torch.save(model, dataset+'_model_'+str(epoch)+'.pt')\n",
        "              \n",
        "  return model\n",
        "\n",
        "def eval_model(model, criterion):\n",
        "  model.to(device)\n",
        "  test_acc=0\n",
        "  pred=[]\n",
        "  true=[]\n",
        "\n",
        "  for j, (inputs, labels) in enumerate(test_loader):\n",
        "    # print(\"j=\",j)\n",
        "    inputs=inputs.to(device)\n",
        "    labels=labels.to(device)\n",
        "\n",
        "    out = model(inputs)\n",
        "    # print(out.shape)\n",
        "    _, predictions = torch.max(out.data, 1)\n",
        "    pred.append(predictions.item())\n",
        "    _,label_max_idx = torch.max(labels.data,1)\n",
        "    true.append(label_max_idx.item())\n",
        "    correct_counts = predictions.eq(label_max_idx)\n",
        "    acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "    test_acc += acc.item() \n",
        "\n",
        "\n",
        "  return true,pred,test_acc/test_x.shape[0]\n",
        "\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "a3V79WSMkFvP"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device=[\"cuda\"]\n",
        "num_epochs = 20\n",
        "vgg16 = train_and_validate(vgg16, loss, optimizer, num_epochs)\n",
        "torch.save(vgg16.state_dict(), 'VGG16.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuoNsm_qkgzx",
        "outputId": "9dca4548-8b10-4d5a-8ca4-a7e58f479af2"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20\n",
            "Epoch : 001, Training: Loss: 0.8590, Accuracy: 69.8750%, \n",
            "\t\tValidation : Loss : 0.9377, Accuracy: 67.5000%, Time: 29.2505s\n",
            "Epoch: 2/20\n",
            "Epoch : 002, Training: Loss: 0.8336, Accuracy: 71.2500%, \n",
            "\t\tValidation : Loss : 0.8756, Accuracy: 70.0000%, Time: 29.1370s\n",
            "Epoch: 3/20\n",
            "Epoch : 003, Training: Loss: 0.8226, Accuracy: 72.6250%, \n",
            "\t\tValidation : Loss : 0.8207, Accuracy: 71.5000%, Time: 29.2138s\n",
            "Epoch: 4/20\n",
            "Epoch : 004, Training: Loss: 0.7902, Accuracy: 71.8750%, \n",
            "\t\tValidation : Loss : 0.8673, Accuracy: 70.5000%, Time: 29.1190s\n",
            "Epoch: 5/20\n",
            "Epoch : 005, Training: Loss: 0.7766, Accuracy: 73.2500%, \n",
            "\t\tValidation : Loss : 0.9351, Accuracy: 69.5000%, Time: 29.1531s\n",
            "Epoch: 6/20\n",
            "Epoch : 006, Training: Loss: 0.8158, Accuracy: 71.2500%, \n",
            "\t\tValidation : Loss : 0.8714, Accuracy: 68.0000%, Time: 29.1577s\n",
            "Epoch: 7/20\n",
            "Epoch : 007, Training: Loss: 0.7366, Accuracy: 74.2500%, \n",
            "\t\tValidation : Loss : 0.8639, Accuracy: 68.5000%, Time: 29.1553s\n",
            "Epoch: 8/20\n",
            "Epoch : 008, Training: Loss: 0.7392, Accuracy: 73.2500%, \n",
            "\t\tValidation : Loss : 0.8157, Accuracy: 74.5000%, Time: 29.1527s\n",
            "Epoch: 9/20\n",
            "Epoch : 009, Training: Loss: 0.6955, Accuracy: 74.8750%, \n",
            "\t\tValidation : Loss : 0.8003, Accuracy: 71.5000%, Time: 29.2085s\n",
            "Epoch: 10/20\n",
            "Epoch : 010, Training: Loss: 0.7368, Accuracy: 74.5000%, \n",
            "\t\tValidation : Loss : 0.8056, Accuracy: 70.5000%, Time: 29.1147s\n",
            "Epoch: 11/20\n",
            "Epoch : 011, Training: Loss: 0.6919, Accuracy: 75.0000%, \n",
            "\t\tValidation : Loss : 0.9165, Accuracy: 67.0000%, Time: 29.1311s\n",
            "Epoch: 12/20\n",
            "Epoch : 012, Training: Loss: 0.7236, Accuracy: 75.0000%, \n",
            "\t\tValidation : Loss : 0.8183, Accuracy: 71.0000%, Time: 29.1080s\n",
            "Epoch: 13/20\n",
            "Epoch : 013, Training: Loss: 0.7038, Accuracy: 74.7500%, \n",
            "\t\tValidation : Loss : 0.9415, Accuracy: 64.0000%, Time: 29.1458s\n",
            "Epoch: 14/20\n",
            "Epoch : 014, Training: Loss: 0.6330, Accuracy: 76.7500%, \n",
            "\t\tValidation : Loss : 0.8136, Accuracy: 68.5000%, Time: 29.1230s\n",
            "Epoch: 15/20\n",
            "Epoch : 015, Training: Loss: 0.6530, Accuracy: 76.6250%, \n",
            "\t\tValidation : Loss : 0.8118, Accuracy: 70.5000%, Time: 29.2001s\n",
            "Epoch: 16/20\n",
            "Epoch : 016, Training: Loss: 0.6149, Accuracy: 79.1250%, \n",
            "\t\tValidation : Loss : 0.9003, Accuracy: 69.5000%, Time: 29.1371s\n",
            "Epoch: 17/20\n",
            "Epoch : 017, Training: Loss: 0.6421, Accuracy: 76.8750%, \n",
            "\t\tValidation : Loss : 0.8647, Accuracy: 73.0000%, Time: 29.1389s\n",
            "Epoch: 18/20\n",
            "Epoch : 018, Training: Loss: 0.6114, Accuracy: 78.6250%, \n",
            "\t\tValidation : Loss : 0.8495, Accuracy: 68.0000%, Time: 29.1235s\n",
            "Epoch: 19/20\n",
            "Epoch : 019, Training: Loss: 0.6000, Accuracy: 77.5000%, \n",
            "\t\tValidation : Loss : 0.7442, Accuracy: 75.0000%, Time: 29.1425s\n",
            "Epoch: 20/20\n",
            "Epoch : 020, Training: Loss: 0.5954, Accuracy: 80.3750%, \n",
            "\t\tValidation : Loss : 0.8523, Accuracy: 70.0000%, Time: 29.1293s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2 b)"
      ],
      "metadata": {
        "id": "5MYZ6wqx1B4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true,y_pred,test_acc= eval_model(vgg16,loss)\n",
        "print(\"Test accuracy for VGG16 model=\",test_acc)\n",
        "print(\"Confustion matrix --> \")\n",
        "print(confusion_matrix(y_true,y_pred))"
      ],
      "metadata": {
        "id": "RMWigKJ4xVIs",
        "outputId": "c61e8f13-3b65-40fa-d136-21a67ecc8e73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy for VGG16 model= 0.74\n",
            "Confustion matrix --> \n",
            "[[ 6  0  1  1  0  0  1  0  0  0]\n",
            " [ 0  8  0  1  1  0  0  0  0  0]\n",
            " [ 1  0 10  1  1  0  1  0  0  0]\n",
            " [ 0  0  0 10  0  0  0  0  0  0]\n",
            " [ 0  0  2  0 14  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  4  0  0  1  1]\n",
            " [ 0  0  2  1  3  0  5  0  0  0]\n",
            " [ 0  0  0  0  0  1  0  4  1  1]\n",
            " [ 0  0  0  0  1  0  0  0  7  0]\n",
            " [ 1  0  0  0  0  0  0  0  0  6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2 c)"
      ],
      "metadata": {
        "id": "FGMhxN7e5zAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_mat= multilabel_confusion_matrix(y_true, y_pred)\n",
        "print(\"Classwise confusion matrix -> \")\n",
        "print(class_mat)\n",
        "for i,arr in enumerate(class_mat):\n",
        "  acc=np.sum(np.diag(arr)) / np.sum(np.sum(arr,axis=1),axis=0)\n",
        "  print(\"classwise accuracy for class \",i,\" =\",acc)"
      ],
      "metadata": {
        "id": "f2hTtz0Y5yNU",
        "outputId": "40d7ff04-40ef-4ccc-97cb-b36886827aa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classwise confusion matrix -> \n",
            "[[[89  2]\n",
            "  [ 3  6]]\n",
            "\n",
            " [[90  0]\n",
            "  [ 2  8]]\n",
            "\n",
            " [[81  5]\n",
            "  [ 4 10]]\n",
            "\n",
            " [[85  5]\n",
            "  [ 0 10]]\n",
            "\n",
            " [[77  6]\n",
            "  [ 3 14]]\n",
            "\n",
            " [[92  1]\n",
            "  [ 3  4]]\n",
            "\n",
            " [[86  3]\n",
            "  [ 6  5]]\n",
            "\n",
            " [[93  0]\n",
            "  [ 3  4]]\n",
            "\n",
            " [[90  2]\n",
            "  [ 1  7]]\n",
            "\n",
            " [[91  2]\n",
            "  [ 1  6]]]\n",
            "classwise accuracy for class  0  = 0.95\n",
            "classwise accuracy for class  1  = 0.98\n",
            "classwise accuracy for class  2  = 0.91\n",
            "classwise accuracy for class  3  = 0.95\n",
            "classwise accuracy for class  4  = 0.91\n",
            "classwise accuracy for class  5  = 0.96\n",
            "classwise accuracy for class  6  = 0.91\n",
            "classwise accuracy for class  7  = 0.97\n",
            "classwise accuracy for class  8  = 0.97\n",
            "classwise accuracy for class  9  = 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(np.sum(matrix,axis=1),axis=0)"
      ],
      "metadata": {
        "id": "TlBrHHuh6Psz",
        "outputId": "e027aa83-2365-4393-d2b6-9348c54288cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=next(iter(test_loader))[1]\n",
        "# z=np.array(y.squeeze(0))\n",
        "# _,id=torch.max(y.data,1)\n",
        "_,idx=torch.max(y.data,1)\n",
        "np.array(idx)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJddXGaVxsLg",
        "outputId": "d7d6989b-1e09-4ebe-bcc9-03ad25dc26d7"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([8])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(iter(test_loader)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "peherRRoxsPb",
        "outputId": "949d507b-fbc8-446d-f277-2e92f68927cb"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-b109e5f192dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: '_SingleProcessDataLoaderIter' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0][0].shape"
      ],
      "metadata": {
        "id": "Kcjn25s8me1J",
        "outputId": "572728ba-dc4e-4511-cdf4-e66345540284",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = Data.DataLoader(FashionMNIST(test_x,test_y,test_transform), shuffle = True)"
      ],
      "metadata": {
        "id": "DCy5BFYimfuQ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=test_loader[99][1]"
      ],
      "metadata": {
        "id": "hWC6CfURnVgT"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "Od0dcJHTnYUo",
        "outputId": "9dd0e628-7227-4804-fb92-7e74a7be006b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CY17tpPsrSdY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}